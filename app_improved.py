"""
æ”¹è¿›åçš„ä¸»åº”ç”¨
(ç²¾ç®€ç‰ˆï¼šä¸“ä¸º 0.5B/1.5B/3B ç­‰è½»é‡çº§æ¨¡å‹è®¾è®¡ï¼Œç§»é™¤é‡åŒ–é€»è¾‘)
"""
import streamlit as st
import os
import time
# ä¸éœ€è¦ torch å’Œ bitsandbytes äº†
from transformers import AutoTokenizer, AutoModelForCausalLM 

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HOME'] = './hf_cache'
os.environ['NO_PROXY'] = "127.0.0.1,localhost"
os.environ['no_proxy'] = "127.0.0.1,localhost"

# å¯¼å…¥æ¨¡å—
from config_manager import get_config, reload_config
from database import DocumentDatabase
from models_with_timeout import (
    load_embedding_model,
    _load_embedding_model_cached,
    _load_generation_model_cached,
)
from milvus_service import get_milvus_client, setup_milvus_collection, index_data_incremental
from services import VectorStoreService, EmbeddingService, GenerationService, DocumentService, RAGService
from security import InputValidator, RateLimiter, ResourceLimiter
from data_utils import load_data

# é¡µé¢é…ç½®
st.set_page_config(layout="wide", page_title="åŒ»ç–— RAG ç³»ç»Ÿ")

# åˆå§‹åŒ–é…ç½®
config = get_config()

# åˆå§‹åŒ–ç»„ä»¶
@st.cache_resource
def init_components_cached(_config):
    return _init_components_internal(_config, None)

def _init_components_internal(_config, progress_placeholder):
    if progress_placeholder:
        progress_placeholder.progress(0.05, text="æ­£åœ¨åˆå§‹åŒ–æ•°æ®åº“...")
    
    db = DocumentDatabase(_config.data.database_path)
    
    if progress_placeholder:
        progress_placeholder.progress(0.1, text="æ­£åœ¨è¿æ¥ Milvus...")
    
    milvus_client = get_milvus_client(_config.milvus.data_path)
    if not milvus_client:
        return None, None, None, None, None, None
    
    if not setup_milvus_collection(milvus_client, _config):
        return None, None, None, None, None, None
    
    def update_progress(value, message):
        if progress_placeholder:
            try:
                progress_placeholder.progress(value, text=message)
            except Exception:
                pass
    
    # åŠ è½½ Embedding
    if progress_placeholder:
        progress_placeholder.progress(0.2, text=f"æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹...")
    embedding_model = load_embedding_model(_config.model.embedding_model_name, update_progress, timeout=600)
    
    if not embedding_model:
        return None, None, None, None, None, None
    
    # =========================================================================
    # ğŸš¨ã€ç²¾ç®€åŠ è½½é€»è¾‘ã€‘ğŸš¨
    # åªé’ˆå¯¹ 0.5B/1.5B/3B æ¨¡å‹ï¼Œä½¿ç”¨æ ‡å‡†æ¨¡å¼åŠ è½½
    # =========================================================================
    model_name_path = _config.model.generation_model_name
    
    if progress_placeholder:
        progress_placeholder.progress(0.5, text=f"æ­£åœ¨åŠ è½½ç”Ÿæˆæ¨¡å‹: {model_name_path}...")

    try:
        # 1. åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(
            model_name_path, 
            trust_remote_code=True
        )
        
        # 2. åŠ è½½æ¨¡å‹ (æ ‡å‡†æ¨¡å¼ - é€Ÿåº¦æœ€å¿«)
        generation_model = AutoModelForCausalLM.from_pretrained(
            model_name_path,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype="auto"  # è®© transformers è‡ªåŠ¨é€‰æ‹©ç²¾åº¦(é€šå¸¸æ˜¯fp16æˆ–fp32)
        )
            
    except Exception as e:
        if progress_placeholder:
            progress_placeholder.progress(1.0, text=f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        print(f"Model Load Error: {e}")
        return None, None, None, None, None, None
    # =========================================================================
    
    if progress_placeholder:
        progress_placeholder.progress(0.8, text="æ­£åœ¨åˆå§‹åŒ–æœåŠ¡å±‚...")
    
    vector_service = VectorStoreService(milvus_client, _config)
    embedding_service = EmbeddingService(embedding_model)
    generation_service = GenerationService(generation_model, tokenizer, _config)
    document_service = DocumentService(db, vector_service, embedding_service)
    rag_service = RAGService(vector_service, embedding_service, generation_service, document_service, _config)
    
    if progress_placeholder:
        progress_placeholder.progress(1.0, text="âœ… åˆå§‹åŒ–å®Œæˆï¼")
        time.sleep(0.5)
        progress_placeholder.empty()
    
    return db, milvus_client, embedding_model, generation_model, tokenizer, rag_service

# æ£€æŸ¥æ˜¯å¦å·²åˆå§‹åŒ–
if 'components_initialized' not in st.session_state or st.sidebar.button("ğŸ”„ é‡æ–°åˆå§‹åŒ–"):
    if 'components_initialized' in st.session_state:
        st.session_state['components_initialized'] = False
        try:
            _load_embedding_model_cached.clear()
        except Exception:
            pass
        config = reload_config()
    
    with st.status("ğŸ”„ ç³»ç»Ÿåˆå§‹åŒ–ä¸­...", expanded=True) as status:
        progress_bar = st.progress(0, text="å¼€å§‹åˆå§‹åŒ–...")
        db, milvus_client, embedding_model, generation_model, tokenizer, rag_service = _init_components_internal(config, progress_bar)
        
        if all([db, milvus_client, embedding_model, generation_model, tokenizer, rag_service]):
            st.session_state['components_initialized'] = True
            st.session_state['db'] = db
            st.session_state['milvus_client'] = milvus_client
            st.session_state['embedding_model'] = embedding_model
            st.session_state['generation_model'] = generation_model
            st.session_state['tokenizer'] = tokenizer
            st.session_state['rag_service'] = rag_service
        else:
            st.error("åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·é‡è¯•")
else:
    db = st.session_state.get('db')
    milvus_client = st.session_state.get('milvus_client')
    embedding_model = st.session_state.get('embedding_model')
    generation_model = st.session_state.get('generation_model')
    tokenizer = st.session_state.get('tokenizer')
    rag_service = st.session_state.get('rag_service')

input_validator = InputValidator(config.security.max_query_length)
rate_limiter = RateLimiter(config.security.rate_limit_per_minute, 60)
resource_limiter = ResourceLimiter(config.security.max_concurrent_queries, config.security.query_timeout)

st.title("ğŸ“„ åŒ»ç–— RAG ç³»ç»Ÿ (0.5B æé€Ÿç‰ˆ)")
st.markdown(f"å½“å‰æ¨¡å‹: `{config.model.generation_model_name}`")

if not all([db, milvus_client, embedding_model, generation_model, tokenizer, rag_service]):
    st.error("âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
    if st.button("ğŸ”„ é‡è¯•"):
        st.session_state['components_initialized'] = False
        st.rerun()
    st.stop()
else:
    st.success("âœ… ç³»ç»Ÿå·²å°±ç»ªï¼")

st.sidebar.header("æ•°æ®ç®¡ç†")
if st.sidebar.button("ç´¢å¼•æ•°æ®"):
    pubmed_data = load_data(config.data.data_file)
    if pubmed_data:
        with st.spinner("æ­£åœ¨ç´¢å¼•æ•°æ®..."):
            index_data_incremental(milvus_client, pubmed_data, embedding_model, db, config)
    else:
        st.sidebar.warning("æ— æ³•åŠ è½½æ•°æ®æ–‡ä»¶")

doc_count = db.get_document_count()
indexed_count = len(db.get_indexed_doc_ids())
st.sidebar.info(f"æ–‡æ¡£æ€»æ•°: {doc_count}\nå·²ç´¢å¼•: {indexed_count}")

st.divider()

with st.expander("ğŸ“œ æŸ¥è¯¢å†å²", expanded=False):
    history = db.get_query_history(limit=10)
    if history:
        for item in history:
            st.text(f"æŸ¥è¯¢: {item['query_text'][:50]}...")
            st.text(f"è€—æ—¶: {item['response_time']:.2f}s")
            st.divider()
    else:
        st.info("æš‚æ— æŸ¥è¯¢å†å²")

query = st.text_input("è¯·è¾“å…¥é—®é¢˜:", key="query_input", placeholder="ä¾‹å¦‚ï¼šä»€ä¹ˆæ˜¯ç³–å°¿ç—…ï¼Ÿ")

if st.button("è·å–ç­”æ¡ˆ", key="submit_button", type="primary"):
    if not query:
        st.warning("è¯·è¾“å…¥é—®é¢˜")
    else:
        is_valid, error_msg = input_validator.validate(query)
        if not is_valid:
            st.error(f"éªŒè¯å¤±è´¥: {error_msg}")
            st.stop()
        
        user_id = "default"
        if not rate_limiter.is_allowed(user_id):
            st.error("è¯·æ±‚è¿‡äºé¢‘ç¹")
            st.stop()
        
        if not resource_limiter.acquire():
            st.error("ç³»ç»Ÿç¹å¿™")
            st.stop()
        
        try:
            start_time = time.time()
            result = rag_service.query(query)
            
            st.subheader("ç”Ÿæˆçš„ç­”æ¡ˆ:")
            st.write(result["answer"])
            
            st.subheader("å‚è€ƒæ–‡æ¡£:")
            for i, doc in enumerate(result["retrieved_docs"]):
                dist = result['distances'][i] if result['distances'] else 0
                with st.expander(f"æ–‡æ¡£ {i+1} (ç›¸ä¼¼åº¦: {dist:.4f})"):
                    st.write(doc['abstract'])
            
            with st.expander("ğŸ“Š æ€§èƒ½"):
                perf = result["performance"]
                st.metric("æ€»è€—æ—¶", f"{perf['total_time']:.2f}s")
        
        except Exception as e:
            st.error(f"å‡ºé”™: {e}")
        finally:
            resource_limiter.release()

st.sidebar.divider()
st.sidebar.text(f"Model: {config.model.generation_model_name}")