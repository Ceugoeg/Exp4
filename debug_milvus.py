"""
Schema ä¿®å¤è„šæœ¬
åŠŸèƒ½ï¼šåˆ é™¤æ—§çš„å­—æ®µåä¸º 'text' çš„æ•°æ®åº“ï¼Œé‡å»ºä¸º 'content_preview'
"""
import os
import json
import time
from pymilvus import connections, utility, Collection, FieldSchema, CollectionSchema, DataType
from sentence_transformers import SentenceTransformer

# ================= é…ç½® =================
os.environ['NO_PROXY'] = "127.0.0.1,localhost"
os.environ['HF_HUB_OFFLINE'] = '1'
current_dir = os.path.dirname(os.path.abspath(__file__))
os.environ['HF_HOME'] = os.path.join(current_dir, 'hf_cache')

# æ¨¡å‹è·¯å¾„
MODEL_PATH = "D:/Code/exp04-easy-rag-system/hf_cache/hub/models--BAAI--bge-small-zh-v1.5/snapshots/7999e1d3359715c523056ef9478215996d62a620"
DATA_FILE = os.path.join(current_dir, "data/processed_data.json")
COLLECTION_NAME = "medical_rag_lite"
DIMENSION = 512
# =======================================

def main():
    print("ğŸš€ 1. è¿æ¥ Milvus...")
    try:
        connections.connect(alias="default", host="127.0.0.1", port="19530")
    except Exception as e:
        print(f"âŒ è¿æ¥å¤±è´¥: {e}")
        return

    # 1. æ— è®ºä»¥å‰å«ä»€ä¹ˆï¼Œç»Ÿç»Ÿåˆ æ‰
    if utility.has_collection(COLLECTION_NAME):
        print(f"ğŸ—‘ï¸ åˆ é™¤æ—§é›†åˆ: {COLLECTION_NAME}")
        utility.drop_collection(COLLECTION_NAME)

    # 2. ä½¿ç”¨æ­£ç¡®çš„å­—æ®µå 'content_preview' é‡å»º
    print(f"ğŸ”¨ 2. é‡å»ºé›†åˆ (Schema ä¿®æ­£)...")
    fields = [
        FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=False),
        FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=DIMENSION),
        # ã€å…³é”®ä¿®æ­£ã€‘è¿™é‡Œå¿…é¡»å« content_previewï¼Œæ‰èƒ½å’Œä¸»ç¨‹åºåŒ¹é…ï¼
        FieldSchema(name="content_preview", dtype=DataType.VARCHAR, max_length=2000)
    ]
    schema = CollectionSchema(fields, "Medical RAG Data")
    col = Collection(COLLECTION_NAME, schema)

    # 3. è¯»å–æ•°æ®
    print(f"ğŸ“‚ 3. è¯»å–æ•°æ®æ–‡ä»¶...")
    try:
        with open(DATA_FILE, 'r', encoding='utf-8') as f:
            data_list = json.load(f)
    except Exception:
        # å¦‚æœæ²¡æ–‡ä»¶ï¼Œé€ ä¸€ç‚¹å‡æ•°æ®ä¿è¯ç¨‹åºèƒ½è·‘
        data_list = [{"title": "æµ‹è¯•", "abstract": "ç³–å°¿ç—…æ˜¯ä¸€ç§ä»£è°¢ç–¾ç—…..."}]

    print("ğŸ“š 4. åŠ è½½æ¨¡å‹...")
    encoder = SentenceTransformer(MODEL_PATH)

    print(f"ğŸ”„ 5. æ­£åœ¨ç”Ÿæˆå‘é‡ ({len(data_list)} æ¡)...")
    
    # å‡†å¤‡æ•°æ®
    ids = []
    texts = []
    previews = []
    
    for i, item in enumerate(data_list[:500]):
        content = f"Title: {item.get('title','')}\nAbstract: {item.get('abstract','')}"
        ids.append(i)
        texts.append(content)
        previews.append(content[:1999])

    embeddings = encoder.encode(texts, normalize_embeddings=True)

    print("ğŸ’¾ 6. å†™å…¥æ–°æ•°æ®...")
    col.insert([ids, embeddings, previews])
    
    print("ğŸš½ æ­£åœ¨å¼ºåˆ¶åˆ·æ–° (Flush)...")
    col.flush() # è¿™ä¸€æ­¥ä¸åšï¼Œä¸»ç¨‹åºå°±æœä¸åˆ°

    print("âš™ï¸ 7. æ„å»ºç´¢å¼•...")
    index_params = {"metric_type": "L2", "index_type": "IVF_FLAT", "params": {"nlist": 128}}
    col.create_index(field_name="embedding", index_params=index_params)
    col.load()

    print(f"âœ… ä¿®å¤å®Œæˆï¼æ•°æ®åº“ç°æœ‰ {col.num_entities} æ¡æ•°æ®ã€‚")
    print("ğŸ‘‰ å­—æ®µåå·²ç»Ÿä¸€ä¸º content_previewï¼Œç°åœ¨é‡å¯ä¸»ç¨‹åºå³å¯ã€‚")

if __name__ == "__main__":
    main()