"""
æ•°æ®è½¬æ¢è„šæœ¬ (ç»ˆæèåˆç‰ˆï¼šå…¨é‡æ–‡ç«  + é—®ç­”å¢å¼º)
åŠŸèƒ½ï¼š
1. ä½¿ç”¨ç¨³å¥çš„é€’å½’åˆ‡åˆ†ç®—æ³•ï¼Œæ‰¾å›æ‰€æœ‰åŒ»ç–—æ–‡ç«  (è§£å†³åªå‰©10ç¯‡çš„é—®é¢˜)
2. è¯»å– medical_questions.jsonï¼Œæ³¨å…¥é—®ç­”å¯¹çŸ¥è¯†
3. è‡ªåŠ¨ç”Ÿæˆ data/processed_data.json ä¾›ä¸»ç¨‹åºç´¢å¼•
"""
import json
import os
from pathlib import Path
import pandas as pd
import re

# ================= é…ç½®å‚æ•° =================
MAX_LENGTH = 1000  # æ–‡ç« åˆ‡åˆ†æœ€å¤§é•¿åº¦
OVERLAP = 100      # ä¸Šä¸‹æ–‡é‡å  (é˜²æ­¢è¯­ä¹‰æ–­è£‚)
# ===========================================

def recursive_split(text, max_len, overlap):
    """
    ç¨³å¥çš„åˆ†å—ç®—æ³• (é€’å½’åˆ‡åˆ†)
    ä¸å†ä¾èµ– "About" æ ‡é¢˜ï¼Œè€Œæ˜¯æŒ‰è‡ªç„¶æ®µè½å’Œå¥å­åˆ‡åˆ†
    """
    if not text: return []
    chunks = []
    start = 0
    text_len = len(text)
    
    while start < text_len:
        end = start + max_len
        if end >= text_len:
            chunks.append(text[start:])
            break
            
        # å¯»æ‰¾æœ€ä½³åˆ‡åˆ†ç‚¹ (ä» end å¾€å‰æ‰¾)
        cut_point = -1
        # ä¼˜å…ˆçº§ï¼šæ¢è¡Œ > å¥å·/æ„Ÿå¹å·/é—®å·
        for i in range(end, start + overlap, -1):
            if text[i] in ['\n', '.', 'ã€‚', '!', '?']:
                cut_point = i + 1
                break
        
        # å¦‚æœæ‰¾ä¸åˆ°æ ‡ç‚¹ï¼Œè¢«è¿«ç¡¬åˆ‡
        if cut_point == -1: 
            cut_point = end
            
        chunks.append(text[start:cut_point].strip())
        # ä¸‹ä¸€å—ä»åˆ‡åˆ†ç‚¹å‡å»é‡å é‡å¼€å§‹
        start = cut_point - overlap
        
    return [c for c in chunks if len(c) > 20] # è¿‡æ»¤è¿‡çŸ­ç¢ç‰‡

def main():
    print("ğŸš€ å¯åŠ¨æ•°æ®è½¬æ¢ç¨‹åº...")
    records = []
    
    # ---------------------------------------------------------
    # é˜¶æ®µ 1: å¤„ç†åŸºç¡€æ–‡ç«  (medical.parquet)
    # ---------------------------------------------------------
    # å®šä¹‰å¯èƒ½çš„è·¯å¾„åˆ—è¡¨ (è‡ªåŠ¨å¯»æ‰¾)
    parquet_paths = [
        "GraphRAG-Benchmark-main/Datasets/Corpus/medical.parquet",
        "medical.parquet",
        "data/medical.parquet"
    ]
    parquet_file = None
    for p in parquet_paths:
        if os.path.exists(p):
            parquet_file = Path(p)
            break
            
    if parquet_file:
        print(f"ğŸ“– [1/2] å¤„ç†æ–‡ç« æº: {parquet_file}")
        try:
            df = pd.read_parquet(parquet_file)
            article_count = 0
            
            for idx, row in df.iterrows():
                context = row.get("context", "")
                if not isinstance(context, str): continue
                
                # ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨é€’å½’åˆ‡åˆ†ï¼Œä¸å†ä¸¢æ•°æ®
                parts = recursive_split(context, MAX_LENGTH, OVERLAP)
                
                for part in parts:
                    # å°è¯•æ™ºèƒ½æå–æ ‡é¢˜ (å–ç¬¬ä¸€è¡Œ)
                    first_line = part.split('\n')[0][:80]
                    # å¦‚æœç¬¬ä¸€è¡Œçœ‹èµ·æ¥åƒæ ‡é¢˜(åŒ…å«About)ï¼Œå°±ç”¨å®ƒï¼Œå¦åˆ™å«ç‰‡æ®µ
                    if "About" in first_line:
                        title = first_line
                    else:
                        title = "Medical Document Fragment"
                    
                    records.append({
                        "title": title,
                        "abstract": part, # å­˜å…¥å®Œæ•´æ­£æ–‡
                        "source_file": "medical.parquet",
                        "chunk_index": len(records)
                    })
                    article_count += 1
            print(f"   => æˆåŠŸæå– {article_count} ä¸ªæ–‡ç« ç‰‡æ®µ (æ•°æ®å®Œæ•´ï¼)")
        except Exception as e:
            print(f"âŒ è¯»å– Parquet å¤±è´¥: {e}")
    else:
        print("âŒ æœªæ‰¾åˆ° medical.parquetï¼Œè¯·æ£€æŸ¥æ–‡ä»¶ä½ç½®ï¼")

    # ---------------------------------------------------------
    # é˜¶æ®µ 2: å¤„ç†é—®ç­”æ•°æ® (medical_questions.json)
    # ---------------------------------------------------------
    # å®šä¹‰å¯èƒ½çš„è·¯å¾„åˆ—è¡¨ (åŒ…å«æ·±å±‚ç›®å½•)
    qa_paths = [
        "GraphRAG-Benchmark-main/Datasets/Questions/medical_questions.json", # æ·±å±‚è·¯å¾„
        "medical_questions.json",                                            # æ ¹ç›®å½•
        "data/medical_questions.json"
    ]
    qa_file = None
    for p in qa_paths:
        if os.path.exists(p):
            qa_file = Path(p)
            break
            
    if qa_file:
        print(f"ğŸ“– [2/2] å¤„ç†é—®ç­”æº: {qa_file}")
        try:
            with open(qa_file, 'r', encoding='utf-8') as f:
                qa_data = json.load(f)
            
            qa_count = 0
            for item in qa_data:
                # å®‰å…¨è¿‡æ»¤ï¼šå†æ¬¡ç¡®ä¿ä¸æ˜¯å°è¯´
                source = str(item.get('source', '')).lower()
                if "novel" in source: 
                    continue
                
                q = item.get('question', '').strip()
                a = item.get('answer', '').strip()
                
                if q and a:
                    # å°†é—®ç­”å¯¹æ ¼å¼åŒ–ä¸ºæ–‡æ¡£å—
                    content = f"Question: {q}\nAnswer: {a}\nEvidence: {item.get('evidence','')}"
                    
                    records.append({
                        "title": f"Q&A: {q[:60]}...",
                        "abstract": content,
                        "source_file": "medical_questions.json",
                        "chunk_index": len(records)
                    })
                    qa_count += 1
            print(f"   => æˆåŠŸæå– {qa_count} ä¸ªé—®ç­”å¯¹")
        except Exception as e:
            print(f"âŒ è¯»å– JSON å¤±è´¥: {e}")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ° medical_questions.jsonï¼Œè·³è¿‡é—®ç­”æ³¨å…¥ã€‚")

    # ---------------------------------------------------------
    # é˜¶æ®µ 3: ä¿å­˜ç»“æœ
    # ---------------------------------------------------------
    output_file = Path("data/processed_data.json")
    output_file.parent.mkdir(exist_ok=True, parents=True)
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(records, f, ensure_ascii=False, indent=2)
        
    print(f"\nâœ… è½¬æ¢å®Œæˆï¼")
    print(f"ğŸ“Š æ€»æ•°æ®é‡: {len(records)} æ¡")
if __name__ == "__main__":
    main()